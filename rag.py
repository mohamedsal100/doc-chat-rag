# -*- coding: utf-8 -*-
"""RAG

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O6CaEVo8M5HWtlPZgKGkNI60fs_0AjmW
"""

!pip install langchain langchain-community transformers sentence-transformers chromadb faiss-cpu pypdf

from pathlib import Path
from typing import List, Tuple
from langchain_community.document_loaders import PyPDFLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma, FAISS
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from langchain.llms import HuggingFacePipeline
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

DEFAULT_EMBED_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
DEFAULT_LLM_MODEL = "google/flan-t5-base"
BASE_DATA_DIR = Path("./data")
PERSIST_DIR = Path("./vector_store")
FAISS_DIR = Path("./faiss_store")

BASE_DATA_DIR.mkdir(parents=True, exist_ok=True)
PERSIST_DIR.mkdir(parents=True, exist_ok=True)
FAISS_DIR.mkdir(parents=True, exist_ok=True)

def read_uploaded_files(paths: List[str]) -> List[Tuple[str, str]]:
    results = []
    for path in paths:
        suffix = Path(path).suffix.lower()
        if suffix == ".pdf":
            loader = PyPDFLoader(path)
            pages = loader.load()
            text = "\n".join([doc.page_content for doc in pages])
        else:
            loader = TextLoader(path, encoding="utf-8")
            docs = loader.load()
            text = "\n".join([d.page_content for d in docs])
        results.append((path, text))
    return results

def chunk_texts(texts: List[str], chunk_size=800, chunk_overlap=120):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n", "\n", ". ", ".", " "]
    )
    chunks = []
    for t in texts:
        chunks.extend(splitter.split_text(t))
    return chunks

def load_embeddings(model_name=DEFAULT_EMBED_MODEL):
    return HuggingFaceEmbeddings(model_name=model_name)

def build_vectorstore(db_type, embeddings, collection_name="rag_collection"):
    if db_type == "Chroma":
        return Chroma(collection_name=collection_name, embedding_function=embeddings, persist_directory=str(PERSIST_DIR))
    elif db_type == "FAISS":
        index_path = FAISS_DIR / "index"
        if (index_path / "index.faiss").exists():
            return FAISS.load_local(str(index_path), embeddings, allow_dangerous_deserialization=True)
        else:
            return FAISS.from_texts([""], embeddings)
    else:
        raise ValueError("Unsupported DB type")

def persist_faiss(vs: FAISS):
    index_path = FAISS_DIR / "index"
    index_path.mkdir(parents=True, exist_ok=True)
    vs.save_local(str(index_path))

def add_chunks_to_store(vs, chunks: List[str], db_type: str):
    if db_type == "FAISS" and len(getattr(vs, "docstore", {})) == 1:
        embs = vs.embedding_function
        vs = FAISS.from_texts(chunks, embs)
        persist_faiss(vs)
        return vs
    else:
        vs.add_texts(chunks)
        if db_type == "Chroma":
            vs.persist()
        elif db_type == "FAISS":
            persist_faiss(vs)
        return vs

def build_hf_llm(model_name=DEFAULT_LLM_MODEL, max_new_tokens=256, temperature=0.1):
    tok = AutoTokenizer.from_pretrained(model_name)
    mdl = AutoModelForSeq2SeqLM.from_pretrained(model_name)
    gen = pipeline(
        "text2text-generation",
        model=mdl,
        tokenizer=tok,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        temperature=temperature,
        repetition_penalty=1.1,
    )
    return HuggingFacePipeline(pipeline=gen)





def build_qa_chain(retriever, llm):
    template = (
        "You are a helpful assistant. Use ONLY the provided context to answer. "
        "If the answer is not in the context, say you don't know.\n\n"
        "Context:\n{context}\n\n"
        "Question: {question}\n"
        "Answer:"
    )
    prompt = PromptTemplate(template=template, input_variables=["context", "question"])
    return RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        chain_type="stuff",
        chain_type_kwargs={"prompt": prompt}
    )

import gradio as gr

embeddings = load_embeddings()
vectorstore = build_vectorstore("FAISS", embeddings)
retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
llm = build_hf_llm()
qa_chain = build_qa_chain(retriever, llm)

def rag_answer(question):
    if not question.strip():
        return "‚ö†Ô∏è Please enter a question", ""

    result = qa_chain({"query": question})
    answer = result.get("result", "(no answer)")
    sources = result.get("source_documents", [])
    sources_text = "\n\n".join(
        [f"{i+1}. {d.page_content[:200]}..." for i, d in enumerate(sources)]
    )
    return answer, sources_text



with gr.Blocks() as demo:
    gr.Markdown("## üß† Retrieval-Augmented Generation (RAG) ‚Äì Q&A")

    q_input = gr.Textbox(label="‚ùì Ask a Question", placeholder="Type your question here...")
    answer_out = gr.Textbox(label="üßæ Answer")
    sources_out = gr.Textbox(label="üìö Sources")

    q_btn = gr.Button("üí¨ Get Answer")
    q_btn.click(fn=rag_answer, inputs=q_input, outputs=[answer_out, sources_out])

demo.launch(share=True)

